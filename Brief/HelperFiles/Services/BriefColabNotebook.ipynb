
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ134Iqjudd7"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade coremltools transformers\n",
        "#might not need transformers datasets torch\n",
        "!pip install transformers datasets torch\n",
        "import torch\n",
        "import coremltools as ct\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load DistilBERT model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"  # Use a DistilBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Create sample input\n",
        "sample_text = \"This is a sample text for tracing.\"\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Define a class to wrap the model (if needed)\n",
        "class ModelWrapper(torch.nn.Module):\n",
        "def __init__(self, model):\n",
        "super(ModelWrapper, self).__init__()\n",
        "self.model = model\n",
        "\n",
        "def forward(self, input_ids, attention_mask):\n",
        "outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "return outputs.logits  # Adjust output based on DistilBERT's structure\n",
        "\n",
        "# Create an instance of the wrapper class\n",
        "model_wrapper = ModelWrapper(model)\n",
        "\n",
        "\n",
        "# Instead of scripting, trace the model\n",
        "traced_model = torch.jit.trace(model_wrapper, (inputs[\"input_ids\"], inputs[\"attention_mask\"]))\n",
        "\n",
        "# Save the traced model\n",
        "traced_model.save(\"distilbert_traced.pt\")\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "\"distilbert_traced.pt\",\n",
        "source=\"pytorch\",\n",
        "inputs=[\n",
        "ct.TensorType(name=\"input_ids\", shape=inputs[\"input_ids\"].shape),\n",
        "ct.TensorType(name=\"attention_mask\", shape=inputs[\"attention_mask\"].shape)\n",
        "],\n",
        "minimum_deployment_target=ct.target.iOS16,\n",
        "convert_to=\"mlprogram\"\n",
        ")\n",
        "\n",
        "\n",
        "# Save the model\n",
        "mlmodel.save(\"distilbert_classification.mlpackage\")\n",
        "\n",
        "!zip -r distilbert_classification.zip distilbert_classification.mlpackage\n",
        "\n",
        "# Download using Colab's files utility\n",
        "from google.colab import files\n",
        "files.download('distilbert_classification.zip')"
      ]
    }
  ]
}
